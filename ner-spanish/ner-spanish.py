# -*- coding: utf-8 -*-
"""final_spanish_ BERT.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/17GlCrfKrdRwmTLu8qCKWsNawBpXLsC2U
"""

#Article Followed: https://towardsdatascience.com/named-entity-recognition-with-bert-in-pytorch-a454405e0b6a

!pip install transformers datasets tokenizers seqeval -q

import pandas as pd
import numpy as np
import torch
from transformers import BertTokenizerFast, BertForTokenClassification #for BERT tokenizers
from transformers import DataCollatorForTokenClassification
from transformers import AutoModelForTokenClassification
from torch.utils.data import DataLoader #for loading batch data
from tqdm import tqdm
from torch.optim import SGD #used for Spanish Dataset
from torch.optim import Adam #used for Twitter Dataset
import csv

#store csv files in google drive, mount to account and use corresponding file directory to read in csv files
from google.colab import drive
drive.mount('/content/drive')

def pandas_reader(fname):
    with open(fname, 'r', encoding='utf-8') as file:
        lines = file.readlines()
    lines = [line.strip() for line in lines[1:]]  # Remove leading/trailing whitespaces
    # Remove consecutive blank lines except for the first one
    lines = [line if line != '' or i == 0 else '<BLANK>' for i, line in enumerate(lines)]
    lines = [line if line != '<BLANK>' else '' for line in lines]  # Convert '<BLANK>' back to ''
    return np.array(lines)

train = pandas_reader('/content/drive/MyDrive/190I/HW4/spanish/train.csv')
val = pandas_reader('/content/drive/MyDrive/190I/HW4/spanish/validation.csv')
test = pandas_reader('/content/drive/MyDrive/190I/HW4/spanish/test_noans.csv')

unique_labels = set(['B-LOC','B-MISC','B-ORG','B-PER','I-LOC','I-MISC','I-ORG','I-PER','O'])

def to_list(data):
    sentences = []
    labels = []
    current_sentence = []
    current_label = []

    count = 0

    for line in data:
        if line == '':
            if current_sentence:
                sentences.append(' '.join(current_sentence))
                labels.append(current_label)
                current_sentence = []
                current_label = []
        else:
            #must account for commas that are character so we can't always just split by comma
            if '"' in line:
              start_index = line.index(',') + 2
              end_index = next((line.index(label, start_index) for label in unique_labels if label in line[start_index:]), len(line))
              end_index = line.rfind(',', start_index, end_index) -1
              characters_between_quotes = line[start_index:end_index]
              count += 1

              current_sentence.append(characters_between_quotes)
              words = line.split(',')
              current_label.append(int(words[-1]))
            else:
              words = line.split(',')  # Split the line by comma
              word = words[1]  # Extract the second element (word)
              current_sentence.append(word)
              current_label.append(int(words[-1]))
              count += 1

    if current_sentence:
        sentences.append(' '.join(current_sentence))
        labels.append(current_label)

    return sentences, labels


#preprocess training data to be dict of two lists, one of sentences and one of labels
train_sents, train_labels = to_list(train)
df_train = {
    "text": train_sents,
    "labels": train_labels
}

#creation of tokenizer using multi-lingual BERT
tokenizer = BertTokenizerFast.from_pretrained('bert-base-multilingual-cased')

from pickle import NONE
label_all_tokens = False
def align_label(texts, labels):
    # print(texts)
    words = texts.split()
    tokenized = []

    tokenized_inputs = tokenizer(texts, padding='max_length', max_length=256, truncation=True)
    word_ids_temp = tokenized_inputs.word_ids()
    subtokens = tokenizer.convert_ids_to_tokens(tokenized_inputs["input_ids"])

    counts = []
    counts.append(1)

    #counts stores the number of subtokens each token was split into once tokemized
    for word in words:
      count = 0
      tokenized = tokenizer(word, padding='max_length', max_length=10, truncation=True)
      sub = tokenizer.convert_ids_to_tokens(tokenized["input_ids"])
      w_ids = tokenized.word_ids()

      for w in w_ids:
        if w is not None:
          count += 1

      counts.append(count)

    word_ids = [None]
    label_ids = []


    #we create our new, correct version of word_ids, using counts
    counts = counts[1:]
    for i, c in enumerate(counts):
      word_ids += [i] * c
      i += c

    #we don't want our token sequence to exceeed length 256
    if len(word_ids) > 256:
      word_ids = word_ids[:256]
    elif len(word_ids) < 256:
      word_ids += [None] * (256-len(word_ids))



    #finally we translate word_ids to our label list using param labels
    for word_idx in word_ids:
          if word_idx is None:
              label_ids.append(-100)
          elif word_idx != previous_word_idx:
              try:
                  label_ids.append(labels[word_idx])
              except:
                  label_ids.append(-100)
          else:
              try:
                  label_ids.append(labels[word_idx] if label_all_tokens else -100)
              except:
                  label_ids.append(-100)
          previous_word_idx = word_idx

    #to be safe and ensure we always have 256 elements, we truncate and return first 256 elements
    return label_ids[0:256]


#our dataframes that we create for our batch processing in training
class DataSequence(torch.utils.data.Dataset):

    def __init__(self, df):

        lb = [list(i) for i in df_train['labels']]
        txt = df['text']
        self.texts = [tokenizer(str(i),
                               padding='max_length', max_length = 256, truncation=True, return_tensors="pt") for i in txt]
        self.labels = [align_label(i,j) for i,j in zip(txt, lb)]

    def __len__(self):

        return len(self.labels)

    def get_batch_data(self, idx):

        return self.texts[idx]

    def get_batch_labels(self, idx):

        return torch.LongTensor(self.labels[idx])

    def __getitem__(self, idx):

        batch_data = self.get_batch_data(idx)
        batch_labels = self.get_batch_labels(idx)

        return batch_data, batch_labels

val_sents, val_labels = to_list(val)
df_val = {
    "text": val_sents,
    "labels": val_labels
}

#creation of BERT model using multi-lingual BERT
class BertModel(torch.nn.Module):

    def __init__(self):

        super(BertModel, self).__init__()

        self.bert = BertForTokenClassification.from_pretrained('bert-base-multilingual-cased', num_labels=len(unique_labels))

    def forward(self, input_id, mask, label):

        output = self.bert(input_ids=input_id, attention_mask=mask, labels=label, return_dict=False)

        return output

#training BERT
def train_BERT(model, df_train, df_val):

    train_dataset = DataSequence(df_train)
    print(train_dataset[0])

    print(" \n SEPARATION \n")
    val_dataset = DataSequence(df_val)
    print(val_dataset[0])

    train_dataloader = DataLoader(train_dataset, num_workers=4, batch_size=BATCH_SIZE, shuffle=True)
    val_dataloader = DataLoader(val_dataset, num_workers=4, batch_size=BATCH_SIZE)

    use_cuda = torch.cuda.is_available()
    device = torch.device("cuda" if use_cuda else "cpu")

    #use SGD optimizer
    optimizer = SGD(model.parameters(), lr=LEARNING_RATE)

    if use_cuda:
        model = model.cuda()


    for epoch_num in range(EPOCHS):

        total_acc_train = 0
        total_loss_train = 0

        model.train()

        #first we want to run model on trained data
        for train_data, train_label in tqdm(train_dataloader):

            train_label = train_label.to(device)
            mask = train_data['attention_mask'].squeeze(1).to(device)
            input_id = train_data['input_ids'].squeeze(1).to(device)

            optimizer.zero_grad()
            loss, logits = model(input_id, mask, train_label)

            for i in range(logits.shape[0]):

              logits_clean = logits[i][train_label[i] != -100]
              label_clean = train_label[i][train_label[i] != -100]

              predictions = logits_clean.argmax(dim=1)
              acc = (predictions == label_clean).float().mean()
              total_acc_train += acc
              total_loss_train += loss.item()

            loss.backward()
            optimizer.step()

        model.eval()

        total_acc_val = 0
        total_loss_val = 0

        #now we want to run model on validation data
        for val_data, val_label in val_dataloader:

            val_label = val_label.to(device)
            mask = val_data['attention_mask'].squeeze(1).to(device)
            input_id = val_data['input_ids'].squeeze(1).to(device)

            loss, logits = model(input_id, mask, val_label)

            for i in range(logits.shape[0]):

              logits_clean = logits[i][val_label[i] != -100]
              label_clean = val_label[i][val_label[i] != -100]

              predictions = logits_clean.argmax(dim=1)
              acc = (predictions == label_clean).float().mean()
              total_acc_val += acc

              total_loss_val += loss.item()

        #print accuracies and losses for each epoch
        print(
            f'Epochs: {epoch_num + 1} | Loss: {total_loss_train / len(df_train["text"]): .3f} | Accuracy: {total_acc_train / len(df_train["text"]): .3f} | Val_Loss: {total_loss_val / len(df_val["text"]): .3f} | Accuracy: {total_acc_val / len(df_val["text"]): .3f}')

#good set of hyperparam for SGD
LEARNING_RATE = 5e-3
EPOCHS = 5
BATCH_SIZE = 32

model = BertModel()

train_BERT(model, df_train, df_val)


def test_to_list(data):
    sentences = []
    current_sentence = []

    count = 0
    for line in data:
        if line == '':
            if current_sentence:
                sentences.append(' '.join(current_sentence))
                current_sentence = []
        else:
            if '"' in line:
                current_sentence.append(',')
                words = line.split(',')
                count += 1
            else:
                words = line.split(',')
                word = words[1]
                current_sentence.append(word)
                count += 1

    if current_sentence:
        sentences.append(' '.join(current_sentence))

    return sentences



#different from to_list func because we have no labels column here
test_sents = test_to_list(test)

#have to ensure tokens are aligned, same as align_labels except this time out max length is 512
def align_word_ids(texts):

    words = texts.split()
    tokenized = []

    tokenized_inputs = tokenizer(texts, padding='max_length', max_length=512, truncation=True)
    word_ids_temp = tokenized_inputs.word_ids()
    subtokens = tokenizer.convert_ids_to_tokens(tokenized_inputs["input_ids"])

    counts = []
    counts.append(1)

    for word in words:
      count = 0
      tokenized = tokenizer(word, padding='max_length', max_length=10, truncation=True)
      sub = tokenizer.convert_ids_to_tokens(tokenized["input_ids"])
      w_ids = tokenized.word_ids()

      for w in w_ids:
        if w is not None:
          count += 1

      counts.append(count)

    word_ids = [None]
    label_ids = []

    counts = counts[1:]
    for i, c in enumerate(counts):

      word_ids += [i] * c
      i += c

    if len(word_ids) > 512:
      word_ids = word_ids[:512]
    elif len(word_ids) < 512:
      word_ids += [None] * (512-len(word_ids))


    # print("word_ids updated:" , word_ids)
    for word_idx in word_ids:
          if word_idx is None:
              label_ids.append(-100)
          elif word_idx != previous_word_idx:
              try:
                  label_ids.append(word_idx)
              except:
                  label_ids.append(-100)
          else:
              try:
                  label_ids.append(word_idx if label_all_tokens else -100)
              except:
                  label_ids.append(-100)
          previous_word_idx = word_idx

    # print("label_ids: ", label_ids)
    return label_ids[0:512]

def evaluate_one_text(model, sentence):


    use_cuda = torch.cuda.is_available()
    device = torch.device("cuda" if use_cuda else "cpu")

    if use_cuda:
        model = model.cuda()

    text = tokenizer(sentence, padding='max_length', max_length = 512, truncation=True, return_tensors="pt")

    mask = text['attention_mask'].to(device)
    input_id = text['input_ids'].to(device)
    label_ids = torch.Tensor(align_word_ids(sentence)).unsqueeze(0).to(device)

    logits = model(input_id, mask, None)
    logits_clean = logits[0][label_ids != -100]

    predictions = logits_clean.argmax(dim=1).tolist()
    prediction_label = [i for i in predictions]
    len_sent = len(sentence.split())
    return prediction_label[:len_sent]

results = []
for i in range(len(test_sents)):
  results += evaluate_one_text(model, test_sents[i])

# Write the data to the CSV file
with open('test_ans.csv', 'w', newline='') as csvfile:
    writer = csv.writer(csvfile)
    writer.writerow(['id', 'label'])  # Write the header row
    for i, label in enumerate(results):
        writer.writerow([i, label])